# StyleFeat: StyleGAN-Based Fine-Grained Expression Manipulation via Action Units
> The code contains the implementation of StyleFeat. The main part of the code is 
> based on pixel2style2pixel [@eladrich](https://github.com/eladrich/pixel2style2pixel).
> And we use [@rosinality]'s implementation of StyleGAN2.

# Train
###Prepare Data
You need to first estimate the AU intensity by [Openface](https://github.com/TadasBaltrusaitis/OpenFace/wiki)
and save it as npy file and save the cerresponding images as npy file too. Moreover, for the 
other pretrained models to be download, you can refer to [pixel2style2pixel](https://github.com/eladrich/pixel2style2pixel)

Run model/D_AU.py to train the AU estimator.

Then, you can train the model by the command:
```angular2html
python scripts/train.py --is_train --dataset_type=ffhq_encode --exp_dir=results --workers=8 --batch_size=4 --test_batch_size=2 --test_workers=8 --val_interval=2500 --save_interval=5000 --encoder_type=GradualStyleEncoder --start_from_latent_avg --lpips_lambda=0.8 --l2_lambda=1 --id_lambda=0.1
```
#Fast Test
For fast test, you only need to download the pretrained [StyleGAN 2](https://github.com/eladrich/pixel2style2pixel).

We provide the 17 AU directions generated by our model 
in the checkpoint directory. You can edit the images sampled
in StyleGAN space by the command:
```angular2html
python3 scripts/train.py --dataset_type=ffhq_encode --exp_dir=results
```
